{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hands-on Deep Learning â€“ Gradient and AutoGrad with TensorFlow/Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This hands-on was created by Thomas Grenier (TensorFlow) and Fabien Millioz (PyTorch), CREATIS, [deepimaging2019](https://deepimaging2019.sciencesconf.org/)_\n",
    "\n",
    "thomas.grenier@creatis.insa-lyon.fr\n",
    "\n",
    "fabien.millioz@creatis.insa-lyon.fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> A - Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:48.994802Z",
     "iopub.status.busy": "2022-07-17T15:24:48.994325Z",
     "iopub.status.idle": "2022-07-17T15:24:50.391890Z",
     "shell.execute_reply": "2022-07-17T15:24:50.391209Z",
     "shell.execute_reply.started": "2022-07-17T15:24:48.994730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 15:24:49.161714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**if no error occurs, your working environment is ok and you can go to next part,\n",
    "> else ... call an assistant for help!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> B - Tensors of TensorFlow   \n",
    "\n",
    "This section presents the tensors of TensorFlow and the link with numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B1- Tensorflow\n",
    "\n",
    "All tensorflow 2 variables are declared and interpreted.\n",
    "As example, a 2 by 2 random unfirom matrix with values in the range [-1;1] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:53.356580Z",
     "iopub.status.busy": "2022-07-17T15:24:53.356055Z",
     "iopub.status.idle": "2022-07-17T15:24:54.311033Z",
     "shell.execute_reply": "2022-07-17T15:24:54.310252Z",
     "shell.execute_reply.started": "2022-07-17T15:24:53.356544Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 15:24:53.358011: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-17 15:24:53.359029: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-17 15:24:53.409941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.410565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-07-17 15:24:53.410602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 15:24:53.412561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-17 15:24:53.412638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-17 15:24:53.414349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-17 15:24:53.414713: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-17 15:24:53.416664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-17 15:24:53.417733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-17 15:24:53.421814: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-17 15:24:53.421948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.422599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.423132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-17 15:24:53.423797: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-17 15:24:53.424072: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-17 15:24:53.424185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.424732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-07-17 15:24:53.424753: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 15:24:53.424775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-17 15:24:53.424788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-17 15:24:53.424801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-17 15:24:53.424814: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-17 15:24:53.424826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-17 15:24:53.424841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-17 15:24:53.424854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-17 15:24:53.424918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.425477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:53.425989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-17 15:24:53.426021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-17 15:24:54.038037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-17 15:24:54.038076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-17 15:24:54.038084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-17 15:24:54.038301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:54.038984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:54.039623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-17 15:24:54.040153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13971 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9045429  0.35481548]\n",
      " [0.5906365  0.51156354]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a_tf = tf.random.uniform(shape=[2, 2], minval=-1.0, maxval=1.0, seed=42)\n",
    "print(a_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:55.026781Z",
     "iopub.status.busy": "2022-07-17T15:24:55.026369Z",
     "iopub.status.idle": "2022-07-17T15:24:55.031760Z",
     "shell.execute_reply": "2022-07-17T15:24:55.031110Z",
     "shell.execute_reply.started": "2022-07-17T15:24:55.026755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.045429  3.5481548]\n",
      " [5.906365  5.1156354]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.multiply(a_tf, 10)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:56.139946Z",
     "iopub.status.busy": "2022-07-17T15:24:56.139281Z",
     "iopub.status.idle": "2022-07-17T15:24:56.144569Z",
     "shell.execute_reply": "2022-07-17T15:24:56.143836Z",
     "shell.execute_reply.started": "2022-07-17T15:24:56.139918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[9.045429  3.5481548]\n",
      " [5.906365  5.1156354]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a_tf * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> B2- Link with numpy\n",
    "\n",
    "We can use numpy array to do the same, and mix tensorflow and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:57.584032Z",
     "iopub.status.busy": "2022-07-17T15:24:57.583651Z",
     "iopub.status.idle": "2022-07-17T15:24:57.591876Z",
     "shell.execute_reply": "2022-07-17T15:24:57.591235Z",
     "shell.execute_reply.started": "2022-07-17T15:24:57.584008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74569327 0.40162969]\n",
      " [0.41960455 0.35185753]]\n",
      "TensorFlow operations convert numpy arrays to Tensors automatically\n",
      "tf.Tensor(\n",
      "[[7.45693267 4.01629691]\n",
      " [4.19604551 3.51857532]], shape=(2, 2), dtype=float64)\n",
      "And NumPy operations convert Tensors to numpy arrays automatically\n",
      "[[8.45693267 5.01629691]\n",
      " [5.19604551 4.51857532]]\n",
      "The .numpy() method explicitly converts a Tensor to a numpy array\n",
      "[[7.45693267 4.01629691]\n",
      " [4.19604551 3.51857532]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a_np = np.random.rand(2, 2)\n",
    "print(a_np)\n",
    "\n",
    "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
    "tensor = tf.multiply(a_np, 10)\n",
    "print(tensor)\n",
    "\n",
    "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
    "print(np.add(tensor, 1))\n",
    "\n",
    "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> C- Calculate a simple gradient\n",
    "We can calculate gradients with automatic differentiation of the function $ c(a) = \\frac{1}{n.m} \\sum_{i,j}^{n,m} 3(a_{ij}+2)^2 $. \n",
    "\n",
    "Let us note $ b(a) = 3(a_{ij}+2)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:24:58.989381Z",
     "iopub.status.busy": "2022-07-17T15:24:58.988886Z",
     "iopub.status.idle": "2022-07-17T15:24:58.999468Z",
     "shell.execute_reply": "2022-07-17T15:24:58.998338Z",
     "shell.execute_reply.started": "2022-07-17T15:24:58.989340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.38277507 0.8182299 ]\n",
      " [0.9155432  0.6904099 ]]\n",
      "b= [[17.032852 23.82726 ]\n",
      " [25.501175 21.714916]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=[2, 2])\n",
    "print(\"a=\", a.numpy())\n",
    "b = 3 * (a + 2) ** 2\n",
    "print(\"b=\", b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define $ c(a) $ the mean of $ b(a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:00.006594Z",
     "iopub.status.busy": "2022-07-17T15:25:00.006191Z",
     "iopub.status.idle": "2022-07-17T15:25:00.011542Z",
     "shell.execute_reply": "2022-07-17T15:25:00.010894Z",
     "shell.execute_reply.started": "2022-07-17T15:25:00.006571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow mean :  tf.Tensor(22.01905, shape=(), dtype=float32)\n",
      "Numpy mean      :  22.01905\n"
     ]
    }
   ],
   "source": [
    "c = tf.reduce_mean(b, name=\"c\")\n",
    "print(\"Tensorflow mean : \", c)\n",
    "print(\"Numpy mean      : \", np.mean(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We remind that the gradient of $c(a)$ is** $ \\nabla c(a) = \\frac{3(a+2)}{2} $.\n",
    "\n",
    "Here we evaluate it automatically from $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:01.208472Z",
     "iopub.status.busy": "2022-07-17T15:25:01.207990Z",
     "iopub.status.idle": "2022-07-17T15:25:01.219649Z",
     "shell.execute_reply": "2022-07-17T15:25:01.218991Z",
     "shell.execute_reply.started": "2022-07-17T15:25:01.208445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TF auto differenciation gradient \n",
      "  tf.Tensor(\n",
      "[[4.0828376 3.378368 ]\n",
      " [3.4646153 3.2258043]], shape=(2, 2), dtype=float32)\n",
      " \n",
      " Manually expressed gradient   \n",
      "  tf.Tensor(\n",
      "[[4.082838  3.378368 ]\n",
      " [3.4646153 3.2258043]], shape=(2, 2), dtype=float32)\n",
      " \n",
      " \n",
      " by the way db_dx =  tf.Tensor(\n",
      "[[16.33135  13.513472]\n",
      " [13.858461 12.903217]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform(shape=[2, 2])\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    b = 3 * (x + 2) ** 2\n",
    "    y = tf.reduce_mean(b)\n",
    "dy_dx = g.gradient(y, x)\n",
    "\n",
    "print(\" TF auto differenciation gradient \\n \", dy_dx)\n",
    "print(\" \\n Manually expressed gradient   \\n \", 3 * (x + 2) / 2)\n",
    "\n",
    "print(\" \\n \\n by the way db_dx = \", g.gradient(b, x))\n",
    "\n",
    "del g  # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> D- Gradient descent example\n",
    "\n",
    "Here we plan to minimize the cost function $L(y, \\hat{y}) = (y - \\hat{y})^2$ according to $w$ and $b$ the weights and biais of a neuron.\n",
    "\n",
    "The gradients $\\frac{\\partial L(y - h(x))}{\\partial w}$ and $\\frac{\\partial L(y - h(x))}{\\partial b}$ are (automatically) computed with:\n",
    "- $h(x) = \\sigma(w.x + b)$\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $L(y, \\hat{y}) = (y - \\hat{y})^2$ (quadratic error)\n",
    "- $y = 0.2$\n",
    "- $x = 1.5$\n",
    "- $b = -2$\n",
    "- $w = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:02.520148Z",
     "iopub.status.busy": "2022-07-17T15:25:02.519772Z",
     "iopub.status.idle": "2022-07-17T15:25:02.531109Z",
     "shell.execute_reply": "2022-07-17T15:25:02.530496Z",
     "shell.execute_reply.started": "2022-07-17T15:25:02.520123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h      =  0.9241418\n",
      "grad b =  0.10153006\n",
      "grad w =  0.15229508\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "    error = (y - h) ** 2  # error = (y - h)**2\n",
    "\n",
    "gradients = g.gradient(error, [b, w])  # error.backward()\n",
    "\n",
    "print(\"h      = \", h.numpy())\n",
    "print(\"grad b = \", gradients[0].numpy())\n",
    "print(\"grad w = \", gradients[1].numpy())\n",
    "del g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We minimize $L(y, h(x))$ iteratively.\n",
    "\n",
    "Weights and bias are updated according to their gradients:\n",
    "\n",
    " - $ w = w - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial w}$ \n",
    "\n",
    " - $ b = b - \\alpha . \\frac{\\partial L(y - h(x))}{\\partial b}$ \n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:04.097633Z",
     "iopub.status.busy": "2022-07-17T15:25:04.097248Z",
     "iopub.status.idle": "2022-07-17T15:25:04.137793Z",
     "shell.execute_reply": "2022-07-17T15:25:04.137117Z",
     "shell.execute_reply.started": "2022-07-17T15:25:04.097609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 error=0.52438 h=0.92414 w=2.84770 b=-2.10153 dE_db=0.10153 dE_dw=0.15230 \n",
      "Epoch 2 error=0.48654 h=0.89753 w=2.65524 b=-2.22984 dE_db=0.12831 dE_dw=0.19246 \n",
      "Epoch 3 error=0.42554 h=0.85233 w=2.40893 b=-2.39404 dE_db=0.16421 dE_dw=0.24631 \n",
      "Epoch 4 error=0.32713 h=0.77195 w=2.10687 b=-2.59542 dE_db=0.20138 dE_dw=0.30206 \n",
      "Epoch 5 error=0.19148 h=0.63758 w=1.80353 b=-2.79765 dE_db=0.20223 dE_dw=0.30334 \n",
      "Epoch 6 error=0.07669 h=0.47693 w=1.59628 b=-2.93582 dE_db=0.13817 dE_dw=0.20726 \n",
      "Epoch 7 error=0.02818 h=0.36786 w=1.47917 b=-3.01388 dE_db=0.07807 dE_dw=0.11710 \n",
      "Epoch 8 error=0.01234 h=0.31107 w=1.40777 b=-3.06149 dE_db=0.04761 dE_dw=0.07141 \n",
      "Epoch 9 error=0.00623 h=0.27892 w=1.36015 b=-3.09323 dE_db=0.03174 dE_dw=0.04762 \n",
      "Epoch 10 error=0.00344 h=0.25865 w=1.32641 b=-3.11572 dE_db=0.02249 dE_dw=0.03374 \n",
      "Epoch 11 error=0.00201 h=0.24488 w=1.30152 b=-3.13232 dE_db=0.01660 dE_dw=0.02490 \n",
      "Epoch 12 error=0.00123 h=0.23504 w=1.28261 b=-3.14492 dE_db=0.01260 dE_dw=0.01890 \n",
      "Epoch 13 error=0.00077 h=0.22776 w=1.26797 b=-3.15469 dE_db=0.00977 dE_dw=0.01465 \n",
      "Epoch 14 error=0.00049 h=0.22223 w=1.25644 b=-3.16237 dE_db=0.00768 dE_dw=0.01152 \n",
      "Epoch 15 error=0.00032 h=0.21794 w=1.24727 b=-3.16849 dE_db=0.00612 dE_dw=0.00917 \n",
      "Epoch 16 error=0.00021 h=0.21457 w=1.23990 b=-3.17340 dE_db=0.00491 dE_dw=0.00737 \n",
      "Epoch 17 error=0.00014 h=0.21189 w=1.23394 b=-3.17737 dE_db=0.00397 dE_dw=0.00596 \n",
      "Epoch 18 error=0.00009 h=0.20975 w=1.22910 b=-3.18060 dE_db=0.00323 dE_dw=0.00485 \n",
      "Epoch 19 error=0.00006 h=0.20801 w=1.22514 b=-3.18324 dE_db=0.00264 dE_dw=0.00396 \n",
      "Epoch 20 error=0.00004 h=0.20660 w=1.22189 b=-3.18541 dE_db=0.00216 dE_dw=0.00325 \n"
     ]
    }
   ],
   "source": [
    "alpha = tf.constant(1.0)  # alpha = 1\n",
    "nb_epochs = 20  # number of epochs\n",
    "# corresponding pyTorch code (Fabien Milloz)\n",
    "x = tf.constant(1.5)  # x = torch.tensor([1.5])\n",
    "y = tf.constant(0.2)  # y = torch.tensor([0.2])\n",
    "b = tf.Variable(-2.0, name=\"b\")  # b = torch.tensor([-2.0], requires_grad=True)\n",
    "w = tf.Variable(3.0, name=\"w\")  # w = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "    with tf.GradientTape(persistent=False) as g:\n",
    "        h = tf.math.sigmoid(w * x + b)  # h = torch.sigmoid(w * x + b)\n",
    "        error = (y - h) ** 2  # error = (y - h)**2\n",
    "    gradients = g.gradient(error, [b, w])\n",
    "    b.assign(b - alpha * gradients[0])\n",
    "    w.assign(w - alpha * gradients[1])\n",
    "    print(\n",
    "        \"Epoch {} error={:.05f} h={:.05f} w={:.05f} b={:.05f} dE_db={:.05f} dE_dw={:.05f} \".format(\n",
    "            i + 1,\n",
    "            error.numpy(),\n",
    "            h.numpy(),\n",
    "            w.numpy(),\n",
    "            b.numpy(),\n",
    "            gradients[0].numpy(),\n",
    "            gradients[1].numpy(),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Question:** Observe the influence of $\\alpha$ for values of 0.01, 0.1, 10 and 100, and try to adapt the number of epochs accordingly.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> E- Hinge Loss example\n",
    "\n",
    "Here we study another loss function for classification : the multiclass Hinge loss. This is the 'SVM' loss.\n",
    "In tensorflow/keras, this loss is available thank to the function tf.keras.losses.CategoricalHinge().\n",
    "\n",
    "  > **Question:** Using Tensorflow online help, give the expression of this loss.\n",
    "    \n",
    "  > **Question:** Give the tensorflow code used to implement this function (explore the source code of tensorflow and more specifically the categorcial_hinge function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:06.281800Z",
     "iopub.status.busy": "2022-07-17T15:25:06.281415Z",
     "iopub.status.idle": "2022-07-17T15:25:06.292340Z",
     "shell.execute_reply": "2022-07-17T15:25:06.291618Z",
     "shell.execute_reply.started": "2022-07-17T15:25:06.281777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred [[0.21194157 0.5761169  0.21194157]]\n",
      "h = 1.3641753\n",
      "y_true shape (1, 3)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1.0, 0.0 , 0.0]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[.0, 1.0, 0.0]], dtype=tf.float32) #before SoftMax  : bSM\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "print(\"y_pred\", y_pred.numpy())\n",
    "h = tf.keras.losses.CategoricalHinge()\n",
    "#h=tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h =\", h( y_true, y_pred ).numpy() )\n",
    "print(\"y_true shape\", y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > **Question:** What do the values of y_pred and y_true mean ?\n",
    "  \n",
    "  > **Question:** With equations of the Hinge loss, verify the previous results.\n",
    "  \n",
    "The loss value is interesting but its derivatives are mandatory to be used within a gradient descent scheme. The following code calculates automatically these derivatives according each y_pred.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:07.516852Z",
     "iopub.status.busy": "2022-07-17T15:25:07.516474Z",
     "iopub.status.idle": "2022-07-17T15:25:07.526560Z",
     "shell.execute_reply": "2022-07-17T15:25:07.525801Z",
     "shell.execute_reply.started": "2022-07-17T15:25:07.516830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.  1.  0.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[-0.28912547  0.36630934 -0.07718389]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred, y_pred_bSM))\n",
    "  y_pred=tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient( h_graph, (y_pred,y_pred_bSM) )\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other realistic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:08.597614Z",
     "iopub.status.busy": "2022-07-17T15:25:08.597234Z",
     "iopub.status.idle": "2022-07-17T15:25:08.607898Z",
     "shell.execute_reply": "2022-07-17T15:25:08.607241Z",
     "shell.execute_reply.started": "2022-07-17T15:25:08.597591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_hinge = 0.926781\n",
      "h_cce = 1.3720545\n",
      "y_pred  [[0.25358546 0.13128695 0.03528275 0.18036644 0.04130046 0.03818236\n",
      "  0.11277747 0.13620754 0.07101061]]\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.constant([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype=tf.float32)\n",
    "#y_pred_bSM = tf.constant([[0.0575, 0.0195, 0.4029, 0.0945, 0.0288, 0.0031, 0.0908, 0.0372, 0.2652]], dtype=tf.float32)\n",
    "y_pred_bSM = tf.constant([[1.26908707619, 0.61077165603, -0.7032195329, 0.92837673425, -0.5457400083, -0.6242400407, 0.45880278945, 0.64756596088, -0.0037845533]], dtype=tf.float32)\n",
    "y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "h_hinge = tf.keras.losses.CategoricalHinge()\n",
    "h_cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(\"h_hinge =\", h_hinge(y_true,y_pred).numpy() )\n",
    "print(\"h_cce =\", h_cce(y_true, y_pred).numpy() )\n",
    "print(\"y_pred \", y_pred.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:09.358371Z",
     "iopub.status.busy": "2022-07-17T15:25:09.357984Z",
     "iopub.status.idle": "2022-07-17T15:25:09.367356Z",
     "shell.execute_reply": "2022-07-17T15:25:09.366684Z",
     "shell.execute_reply.started": "2022-07-17T15:25:09.358347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.  0.  0.  1.  0.  0.  0.  0.  0.]], shape=(1, 9), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.23501818  0.0096127   0.00258337  0.1935727   0.00302398  0.00279568\n",
      "   0.00825746  0.00997298  0.00519933]], shape=(1, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch((y_pred,y_pred_bSM))\n",
    "  y_pred = tf.nn.softmax(y_pred_bSM)\n",
    "  h_graph = h_hinge(y_true, y_pred) # <=> tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, (y_pred,y_pred_bSM))\n",
    "print(dh_dy[0])\n",
    "print(dh_dy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T15:25:09.793381Z",
     "iopub.status.busy": "2022-07-17T15:25:09.793033Z",
     "iopub.status.idle": "2022-07-17T15:25:09.801138Z",
     "shell.execute_reply": "2022-07-17T15:25:09.800442Z",
     "shell.execute_reply.started": "2022-07-17T15:25:09.793359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.9434438  1.         1.         1.         1.         1.\n",
      "   1.         1.         1.       ]], shape=(1, 9), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(y_pred)\n",
    "  h_graph = h_cce(y_true, y_pred)\n",
    "  dh_dy = g.gradient(h_graph, y_pred)\n",
    "print(dh_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
